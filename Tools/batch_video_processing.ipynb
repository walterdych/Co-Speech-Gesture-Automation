{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca871f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056b65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34058d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe components\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1708ae85-d816-4dee-9385-7e8c216055c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a video file\n",
    "def process_video(video_path):\n",
    "    data = []\n",
    "    with mp_holistic.Holistic(static_image_mode=False, model_complexity=MODEL, \n",
    "                              min_detection_confidence=0.4, min_tracking_confidence=0.4) as holistic:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, image = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            time_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "\n",
    "            # Append pose landmarks data\n",
    "            if results.pose_landmarks is not None:\n",
    "                right_shoulder_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].x\n",
    "                right_shoulder_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].y\n",
    "                left_shoulder_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER].x\n",
    "                left_shoulder_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER].y\n",
    "                right_elbow_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW].x\n",
    "                right_elbow_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW].y\n",
    "                left_elbow_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW].x\n",
    "                left_elbow_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW].y\n",
    "                right_wrist_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST].x\n",
    "                right_wrist_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST].y\n",
    "                left_wrist_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_WRIST].x\n",
    "                left_wrist_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_WRIST].y\n",
    "                right_eye_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_EYE].x\n",
    "                right_eye_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_EYE].y\n",
    "                left_eye_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EYE].x\n",
    "                left_eye_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EYE].y\n",
    "                nose_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x\n",
    "                nose_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y\n",
    "                right_index_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_INDEX].x\n",
    "                right_index_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_INDEX].y\n",
    "\n",
    "                data.append([time_ms, right_shoulder_x, right_shoulder_y, left_shoulder_x, left_shoulder_y, \n",
    "                             right_elbow_x, right_elbow_y, left_elbow_x, left_elbow_y, \n",
    "                             right_wrist_x, right_wrist_y, left_wrist_x, left_wrist_y, \n",
    "                             right_eye_x, right_eye_y, left_eye_x, left_eye_y, right_index_x, right_index_y,\n",
    "                             nose_x, nose_y])\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9302910f-3102-4290-8117-c22677242f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as /n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_G_VCV_IN_1_keypoints.csv\n",
      "DataFrame saved as /n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_G_CVCV_IN_keypoints.csv\n",
      "DataFrame saved as /n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_G_VCV_IN_2_keypoints.csv\n",
      "DataFrame saved as /n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_G_CVC_keypoints.csv\n",
      "DataFrame saved as /n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_G_VCV_FIN_keypoints.csv\n",
      "DataFrame saved as /n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_G_CVCV_FIN_keypoints.csv\n"
     ]
    }
   ],
   "source": [
    "# Worker function to process a single video\n",
    "def process_video_file(video_path):\n",
    "    video_data = process_video(video_path)\n",
    "    df = pd.DataFrame(video_data, columns=[\n",
    "    \"time_ms\", \n",
    "    \"right_shoulder_x\", \"right_shoulder_y\", \n",
    "    \"left_shoulder_x\", \"left_shoulder_y\", \n",
    "    \"right_elbow_x\", \"right_elbow_y\", \n",
    "    \"left_elbow_x\", \"left_elbow_y\", \n",
    "    \"right_wrist_x\", \"right_wrist_y\", \n",
    "    \"left_wrist_x\", \"left_wrist_y\", \n",
    "    \"right_eye_x\", \"right_eye_y\", \n",
    "    \"left_eye_x\", \"left_eye_y\",\n",
    "    \"nose_x\", \"nose_y\",\n",
    "    \"right_index_x\", \"right_index_y\" \n",
    "    ])\n",
    "    csv_file_name = \"/n/kfranich_speech_l3/Lab/6000_EMA/processed_articulatory_data/video_processing/keypoints/SN6004_ARTGEST_ENG_VIDAUD/\" + os.path.splitext(os.path.basename(video_path))[0] + \"_keypoints.csv\"\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    return f\"DataFrame saved as {csv_file_name}\"\n",
    "\n",
    "# Main script to process all MOV files in a directory using multiprocessing\n",
    "def main(directory_path):\n",
    "    video_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith(\".MOV\")]\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_video_file, video_files)\n",
    "\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"/n/kfranich_speech_l3/Lab/6000_EMA/video_and_audio_data/SN6004_ARTGEST_ENG_VIDAUD/SN6004_ARTGEST_ENG_VIDAUD_GEST\"\n",
    "    main(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e47dc-136d-4488-a17d-1930dda89229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kfranich_lab]",
   "language": "python",
   "name": "conda-env-kfranich_lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
